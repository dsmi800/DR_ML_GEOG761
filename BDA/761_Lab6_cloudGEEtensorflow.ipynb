{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4c0ff6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 6: Earth Engine on the Google Cloud Platform\n",
    "\n",
    "- In this lab we put together the all the skills we have been developing during the course.\n",
    "- The objective is to achieve an end-to-end satellite data machine learning workflow in a GCP.\n",
    "- Our workflow is:\n",
    "    1) Connect the final requied APIs\n",
    "    2) Increase the GPU quota of the project\n",
    "    3) Create a sufficently powerful instance\n",
    "    4) Create and connect data storage\n",
    "    5) Carry out end-to-end ML\n",
    "    6) Stop the instance to avoid ongoing costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19d506",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources\n",
    "> https://github.com/google/earthengine-community/blob/master/guides/linked/TF_demo1_keras.ipynb\n",
    "\n",
    "> https://colab.research.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/main/people-and-planet-ai/land-cover-classification/cloud-tensorflow.ipynb#scrollTo=G7-JZgREZHQK\n",
    "\n",
    "> https://cloud.google.com/storage/docs/locations\n",
    "\n",
    "> https://medium.com/@yvanjaquino/earth-engine-authentication-in-google-cloud-python-edition-6df832874f62\n",
    "\n",
    "> https://cloud.google.com/vertex-ai/docs/workbench/instances/cloud-storage\n",
    "\n",
    "> https://docs.google.com/presentation/d/1HcbbEnC0wbGfp-d6qXbVkcBryP0acNlBKeMCsDCUXpw/edit#slide=id.g15d38b5130c_10_3150\n",
    "\n",
    "> https://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_AI_Platform.ipynb\n",
    "\n",
    "> https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_tree_counting_model.ipynb\n",
    "\n",
    "> https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_TensorFlow_DNN_from_scratch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef58a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1) Connect the APIs\n",
    "- As ever, make sure you are in the ML project you set up previously (i.e. not your purely GEE projects).\n",
    "- We already have the Vertex and GEE APIs connected. But there are two more we need for this. Apply them to your project as we did previously:\n",
    "    - Cloud Run API\n",
    "    - Dataflow API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fe726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Increase GPU quota\n",
    "- Go to the IAM and admin tab of your dashboard, the 'Quotas' submenu of that.\n",
    "- Use the filters slot to get the general GPU limits:-\n",
    "    - The filter you want to set is GPUs (all regions). Then select the tick-box of the 'Compute Engine API' as below:\n",
    "![](images/quotafilters.jpg)\n",
    "- With it selected, click 'Edit Quotas' and set it to 1. With the reason:\n",
    "    - \"I am attending a course at the University of Auckland, 761 Satellite Data and Machine Learning. We are signed up to the Google Education program and we are now at the final stage, whereby we need to spin up an instance we can then do an end-to-end pipeline, that uses a FCNN/CNN for classification tasks. The course ends in 5 weeks, and the instances will not be in use after this.\"\n",
    "    \n",
    "### DO THIS NOW! ASAP\n",
    "- Mine came through instantly, once approved it can take up to 15mins to appear in your quota allocation.\n",
    "- However it may take longer... so get it done!\n",
    "- You get an email when you submit the request and one when it is approved.\n",
    "- Refresh the quota page of your dashboard to check if it has been increased. Will change from 0 to 1.\n",
    "\n",
    "For more detail, and screenshots, see this stack overflow post:\n",
    ">https://stackoverflow.com/questions/53415180/gcp-error-quota-gpus-all-regions-exceeded-limit-0-0-globally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3420",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3) Set up the instance\n",
    "Here we need to use the advanced settings in order to have sufficent compute available to us. The required settings are...\n",
    "\n",
    "- Details:\n",
    "    - Region: us-central1\n",
    "        - This is the region that hosts GEE data and therefore is most efficent for the pipeline in this lab. If you were mostly uploading data, then Australia would be the best bet.\n",
    "        - Sometimes cannot get a instance in a given region as too busy, may have to shop around the regions.\n",
    "    - Workbench type: Instance\n",
    "- Environment:\n",
    "    - Use the latest version\n",
    "- Machine type: may have to adjust these depending on the region you end up in.\n",
    "    - n1-highmem-16(16vCPUs, 104 GB RAM)\n",
    "    - GPU: NVIDIA T4\n",
    "    - Number of GPUs: 1\n",
    "    - Install NVIDIA GPU driver automatically for me: Check yes.\n",
    "    - Leave the defaults as they are for the Shielded VM and Idle shutdown options\n",
    "- Disks:\n",
    "    - Data disk size in GB: 200 GB\n",
    "        - You have seen the size required for many of the prior work that you are looking at using... when doing this for real, consider carefully this size requirement!\n",
    "        - It is not for long-term storage (that is what your bucket is for), but it is for intermediate processing files, data that has to be loaded into memory, the size of the model etc etc. Anything that is stored or needed during the processing.\n",
    "    - Otherwise leave as defaults\n",
    "- Networking:\n",
    "    - Leave as defaults\n",
    "- IAM and security:\n",
    "    - Leave as defaults\n",
    "- System health:\n",
    "    - Leave as defaults\n",
    "    \n",
    "With this complete, hit 'Create'!\n",
    "> I also deleted my first test instance, just to free up any quota it was taking up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40240c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4) Create a storage bucket\n",
    "### Key concepts\n",
    "- You permanently set a geographic location for storing your object data when you create a bucket.\n",
    "- You cannot change a bucket's location after it's created, but you can move your data to a bucket in a different location.\n",
    "- You can select from the following location types:\n",
    "    - A region is a specific geographic place, such as São Paulo.\n",
    "    - A dual-region is a specific pair of regions, such as Tokyo and Osaka.\n",
    "    - A multi-region is a large geographic area, such as the United States, that contains two or more geographic places.\n",
    "- The location type determines how you data is replicated and priced.\n",
    "- The location information for a bucket is part of the bucket's metadata, which you can view if you have permission to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad8f54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Set-up considerations\n",
    "- Put in the same region that your working instance is going to be in. Minimises data movement time and costs. \n",
    "- Putting it in just the one region is cheapest options and perfectly fine for our purposes.\n",
    "- Prefer locations that are geographically closer to you with low carbon emissions, highlighted with the Leaf icon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509e9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get the bucket set up\n",
    "- Check that are still in the relevant project (remember that assets are not available between projects, only within projects).\n",
    "- Go to Cloud Storag via the three line option menu top left.\n",
    "- + Create\n",
    "- Pick a name that you can type...\n",
    "    - Mine is ml-satdata-bucket-001, but has to be globally unique!\n",
    "- Pick region, same as where you have set up the instance.\n",
    "    - Australia-southeast1 (Sydney) in my case.\n",
    "- Set to Autoclass\n",
    "- Leave access control as the defaults\n",
    "- Leave protection tools as the defaults.\n",
    "- Make a note of the location pricing at the bottom once you have set all these options. This will give you an idea of the impact on your budget.\n",
    "    - Mine is at 0.0025 to 0.023 USD per GB/month\n",
    "    - With 0.025 USD per objects per month additional cost\n",
    "        - This structure means that they will charge you more if you are storing LOADS of small things. It's a memory optimization thing.\n",
    "- Confirm that you do not want public access enabled to your data.\n",
    "\n",
    "Once set up, mine looks like this:\n",
    "![](images/bucket.jpg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915629da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data movement into buckets\n",
    "- Can be done manually via the 'UPLOAD FILES' button the dashboard as in the image above.\n",
    "- Programmatically from your local to the bucket, using a console/command line/IDE that can see your local file structure and has internet access.\n",
    "- Programmatically from a source accesible via an internet connection, using the console/notebook/command line with internet access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b54734",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Access your cloud storage in the Vertex AI instance notebook\n",
    "- Click and drag the left pane, the one with the file names of the notebooks, to the right. This will reveal a menu at the top that includes the 'mount shared storage' button. \n",
    "    - Yes, I spent an hour looking for this button even having read the doc linked below. Sigh.\n",
    "- Mount your bucket:\n",
    "    - Do so by clicking on the mount storage, 3 lines, icon\n",
    "    - Enter the name of your bucket, ml-satdata-bucket-001 in my case.\n",
    "- You should now be able to see the bucket as a file in your instance.\n",
    "\n",
    "> https://cloud.google.com/vertex-ai/docs/workbench/managed/cloud-storage\n",
    "\n",
    "Should look something like this once successful (minus my randon code to the right of course):\n",
    "![](images/bukcetmount.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9c7f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5) End-to-end ML\n",
    "We will be following the tutorial and dataset from the source linked at the start of this lab. Modified to our ends! From this point forwards, I provide blocks of code for you to copy into your GCP notebook and modify with your specifics.\n",
    "\n",
    "- Activate your instance and open up the Jupyter Notebook interface.\n",
    "- Select the TensorFlow 2-11 notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e999c-e528-49ea-a911-57f939250b60",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is an Earth Engine <> TensorFlow demonstration notebook. Specifically, this notebook shows:\n",
    "\n",
    "1. Exporting training/testing data from Earth Engine in TFRecord format.\n",
    "2. Preparing the data for use in a TensorFlow model.\n",
    "3. Training and validating a simple model (Keras Sequential neural network) in TensorFlow.\n",
    "4. Making predictions on image data exported from Earth Engine in TFRecord format.\n",
    "5. Ingesting classified image data to Earth Engine in TFRecord format.\n",
    "\n",
    "This is intended to demonstrate a complete i/o pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a9e0c-256f-42cd-aa4c-8cb16e5502b0",
   "metadata": {},
   "source": [
    "### A quick VertexAI OS diversion...\n",
    "- What operating system are we on here... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5e680a-656d-42c0-8434-d069f363cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter\n"
     ]
    }
   ],
   "source": [
    "#bash...\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b3aeb-9e14-4bd9-8a2d-8cb2181e5dd5",
   "metadata": {},
   "source": [
    "## Setup software libraries\n",
    "Import software libraries and/or authenticate as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326d8381-fc35-44a9-acfb-893324526f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress annoying TensorFlow warnings\n",
    "TF_CPP_MIN_LOG_LEVEL=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdb40d2-badd-4799-91d6-362c6080c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 01:34:27.710268: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 01:34:28.640972: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-18 01:34:28.641084: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-09-18 01:34:28.641099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import the TensorFlow library and check the version.\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdddf68-b13b-48c8-a56f-1eac174019ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "# We will use the Folium library for visualization. Import the library and check the version.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceed5905-31e3-42ed-8a3a-829aef3e06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is bae\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8384e29-179e-497a-b59c-8deeedbdb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install earthengine-api\n",
    "\n",
    "import ee\n",
    "from google.auth import compute_engine\n",
    "\n",
    "scopes = [\n",
    "    \"https://www.google.apis.com/auth/earthengine\"\n",
    "]\n",
    "\n",
    "credentials = compute_engine.Credentials(scopes=scopes)\n",
    "ee.Initialize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861b267-c342-4771-8e99-d99477ca9687",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "This set of global variables will be used throughout. For this demo, you must have a Cloud Storage bucket into which you can write files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028a976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Earth Engine username.  This is used to import a classified image\n",
    "# into your Earth Engine assets folder.\n",
    "USER_NAME = 'dsmi800'\n",
    "\n",
    "# Cloud Storage bucket into which training, testing and prediction \n",
    "# datasets will be written.  You must be able to write into this bucket.\n",
    "OUTPUT_BUCKET = 'ml-bucket-dsmi800'\n",
    "\n",
    "# Use Landsat 8 surface reflectance data for predictors.\n",
    "L8SR = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "# Use these bands for prediction.\n",
    "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "\n",
    "# This is a training/testing dataset of points with known land cover labels.\n",
    "LABEL_DATA = ee.FeatureCollection('projects/google/demo_landcover_labels')\n",
    "# The labels, consecutive integer indices starting from zero, are stored in\n",
    "# this property, set on each point.\n",
    "LABEL = 'landcover'\n",
    "# Number of label values, i.e. number of classes in the classification.\n",
    "N_CLASSES = 3\n",
    "\n",
    "# These names are used to specify properties in the export of\n",
    "# training/testing data and to define the mapping between names and data\n",
    "# when reading into TensorFlow datasets.\n",
    "FEATURE_NAMES = list(BANDS)\n",
    "FEATURE_NAMES.append(LABEL)\n",
    "\n",
    "# File names for the training and testing datasets.  These TFRecord files\n",
    "# will be exported from Earth Engine into the Cloud Storage bucket.\n",
    "TRAIN_FILE_PREFIX = 'Training_demo'\n",
    "TEST_FILE_PREFIX = 'Testing_demo'\n",
    "file_extension = '.tfrecord.gz'\n",
    "TRAIN_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
    "TEST_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
    "\n",
    "# File name for the prediction (image) dataset.  The trained model will read\n",
    "# this dataset and make predictions in each pixel.\n",
    "IMAGE_FILE_PREFIX = 'Image_pixel_demo_'\n",
    "\n",
    "# The output path for the classified image (i.e. predictions) TFRecord file.\n",
    "OUTPUT_IMAGE_FILE = 'gs://' + OUTPUT_BUCKET + '/Classified_pixel_demo.TFRecord'\n",
    "# Export imagery in this region.\n",
    "EXPORT_REGION = ee.Geometry.Rectangle([-122.7, 37.3, -121.8, 38.00])\n",
    "# The name of the Earth Engine asset to be created by importing\n",
    "# the classified image from the TFRecord file in Cloud Storage.\n",
    "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/Classified_pixel_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6babe07-b36e-4ac5-9b9a-12750b427f2c",
   "metadata": {},
   "source": [
    "## Get Training and Testing data from Earth Engine\n",
    "To get data for a classification model of three classes (bare, vegetation, water), we need labels and the value of predictor variables for each labeled example. We've already generated some labels in Earth Engine. Specifically, these are visually interpreted points labeled \"bare,\" \"vegetation,\" or \"water\" for a very simple classification demo (as we did in our random forest lab). For predictor variables, we'll use Landsat 8 surface reflectance imagery, bands 2-7.\n",
    "\n",
    "> Google hand labelling example: https://code.earthengine.google.com/?scriptPath=Examples%3ADemos%2FClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94288655-bdf7-4dba-b477-43ba8d415191",
   "metadata": {},
   "source": [
    "### Prepare Landsat 8 imagery\n",
    "First, make a cloud-masked median composite of Landsat 8 surface reflectance imagery from 2018. Check the composite by visualizing with folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65f155f-7442-4067-9ce9-341cfa84cbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_4138589a25040c576960f9f65c1af570 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_4138589a25040c576960f9f65c1af570&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_4138589a25040c576960f9f65c1af570 = L.map(\n",
       "                &quot;map_4138589a25040c576960f9f65c1af570&quot;,\n",
       "                {\n",
       "                    center: [38.0, -122.5],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_b1b5dea7862542d3913822a6b85d6c70 = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_4138589a25040c576960f9f65c1af570);\n",
       "        \n",
       "    \n",
       "            var tile_layer_1c432223b0ef9e6ecac5d471fb269f27 = L.tileLayer(\n",
       "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/d3a4c11c68bba9946d4ed3b8e6a16fa8-8c910c17636299c5d8d3de372f487290/tiles/{z}/{x}/{y}&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Map Data \\u00a9 Google Earth Engine&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_4138589a25040c576960f9f65c1af570);\n",
       "        \n",
       "    \n",
       "            var layer_control_21ce02e1ceda0541d95cfe3829df29cb = {\n",
       "                base_layers : {\n",
       "                    &quot;openstreetmap&quot; : tile_layer_b1b5dea7862542d3913822a6b85d6c70,\n",
       "                },\n",
       "                overlays :  {\n",
       "                    &quot;median composite&quot; : tile_layer_1c432223b0ef9e6ecac5d471fb269f27,\n",
       "                },\n",
       "            };\n",
       "            L.control.layers(\n",
       "                layer_control_21ce02e1ceda0541d95cfe3829df29cb.base_layers,\n",
       "                layer_control_21ce02e1ceda0541d95cfe3829df29cb.overlays,\n",
       "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
       "            ).addTo(map_4138589a25040c576960f9f65c1af570);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f183214bbb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cloud masking function.\n",
    "def maskL8sr(image):\n",
    "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "  qa = image.select('pixel_qa')\n",
    "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "  return image.updateMask(mask).select(BANDS).divide(10000)\n",
    "\n",
    "# The image input data is a 2018 cloud-masked median composite.\n",
    "image = L8SR.filterDate('2018-01-01', '2018-12-31').map(maskL8sr).median()\n",
    "\n",
    "# Use folium to visualize the imagery.\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
    "map = folium.Map(location=[38., -122.5])\n",
    "\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data © Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680f1a6-17a3-4cdc-8f11-f705098da432",
   "metadata": {},
   "source": [
    "### Add pixel values of the composite to labeled points\n",
    "Some training labels have already been collected for you. Load the labeled points from an existing Earth Engine asset. Each point in this table has a property called landcover that stores the label, encoded as an integer. Here we overlay the points on imagery to get predictor variables along with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c305c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': {'geometry': None,\n",
      "              'id': '000066e7d9bc84b3f95d_0',\n",
      "              'properties': {'B2': 0.049150001257658005,\n",
      "                             'B3': 0.06965000182390213,\n",
      "                             'B4': 0.08974999934434891,\n",
      "                             'B5': 0.1729000061750412,\n",
      "                             'B6': 0.2125999927520752,\n",
      "                             'B7': 0.15150000154972076,\n",
      "                             'landcover': 1,\n",
      "                             'random': 0.5484198857675888},\n",
      "              'type': 'Feature'}}\n",
      "{'testing': {'geometry': None,\n",
      "             'id': '00009f65e3c9ae02b84e_0',\n",
      "             'properties': {'B2': 0.05220000073313713,\n",
      "                            'B3': 0.062049999833106995,\n",
      "                            'B4': 0.03660000115633011,\n",
      "                            'B5': 0.01140000019222498,\n",
      "                            'B6': 0.006800000090152025,\n",
      "                            'B7': 0.005249999929219484,\n",
      "                            'landcover': 2,\n",
      "                            'random': 0.8210157114829347},\n",
      "             'type': 'Feature'}}\n"
     ]
    }
   ],
   "source": [
    "# Sample the image at the points and add a random column.\n",
    "sample = image.sampleRegions(\n",
    "  collection=LABEL_DATA, properties=[LABEL], scale=30).randomColumn()\n",
    "\n",
    "# Partition the sample approximately 70-30. (No validate, you have seen how to change this in other labs...)\n",
    "training = sample.filter(ee.Filter.lt('random', 0.7))\n",
    "testing = sample.filter(ee.Filter.gte('random', 0.7))\n",
    "\n",
    "# Pprint == 'pretty print'\n",
    "from pprint import pprint\n",
    "\n",
    "# Print the first couple points to verify.\n",
    "pprint({'training': training.first().getInfo()})\n",
    "pprint({'testing': testing.first().getInfo()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22540937-1b9d-49f0-b235-2f37400ff317",
   "metadata": {},
   "source": [
    "### Export the training and testing data\n",
    "Now that there's training and testing data in Earth Engine and you've inspected a couple examples to ensure that the information you need is present, it's time to materialize the datasets in a place where the TensorFlow model has access to them. You can do that by exporting the training and testing datasets to tables in TFRecord format in your Cloud Storage bucket.\n",
    "\n",
    "> Learn more about TF records: https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf08ab4d-59c2-4f44-9b0d-8746e523ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you can see the output bucket.  You must have write access.\n",
    "print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
    "    else 'Can not find output Cloud Storage bucket.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4abfc-9fec-48cb-abe2-9454c74b5a7e",
   "metadata": {},
   "source": [
    "Once you've verified the existence of the intended output bucket, run the exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927f338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tasks.\n",
    "training_task = ee.batch.Export.table.toCloudStorage(\n",
    "  collection=training,\n",
    "  description='Training Export',\n",
    "  fileNamePrefix=TRAIN_FILE_PREFIX,\n",
    "  bucket=OUTPUT_BUCKET,\n",
    "  fileFormat='TFRecord',\n",
    "  selectors=FEATURE_NAMES)\n",
    "\n",
    "testing_task = ee.batch.Export.table.toCloudStorage(\n",
    "  collection=testing,\n",
    "  description='Testing Export',\n",
    "  fileNamePrefix=TEST_FILE_PREFIX,\n",
    "  bucket=OUTPUT_BUCKET,\n",
    "  fileFormat='TFRecord',\n",
    "  selectors=FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f2647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the tasks.\n",
    "training_task.start()\n",
    "testing_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492eeed-fe46-42f9-8e14-049680ab2229",
   "metadata": {},
   "source": [
    "### Monitor task progress\n",
    "You can see all your Earth Engine tasks by listing them. Make sure the training and testing tasks are completed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa41507-2630-4899-a854-5c852fb895f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Task A4XN4AFRVCZPDRAKH3UZAXU7 EXPORT_FEATURES: Testing Export (READY)>,\n",
      " <Task T3WR2ISKKUBAAANUACXJVMTJ EXPORT_FEATURES: Training Export (READY)>,\n",
      " <Task VEP6L2JTWSPDKQOIOHJHMQHI EXPORT_FEATURES: Testing Export (COMPLETED)>,\n",
      " <Task JCQAFODL2KTZLAGSMNH33JKE EXPORT_FEATURES: Training Export (COMPLETED)>]\n"
     ]
    }
   ],
   "source": [
    "# Print all tasks.\n",
    "pprint(ee.batch.Task.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6dd76-a903-4e2b-8727-05033e98dc8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check existence of the exported files\n",
    "If you've seen the status of the export tasks change to COMPLETED, then check for the existence of the files in the output Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad25387",
   "metadata": {},
   "source": [
    "### Check existence of the exported files\n",
    "If you've seen the status of the export tasks change to COMPLETED, then check for the existence of the files in the output Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb9b27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found training file.\n",
      "Found testing file.\n"
     ]
    }
   ],
   "source": [
    "print('Found training file.' if tf.io.gfile.exists(TRAIN_FILE_PATH) \n",
    "    else 'No training file found.')\n",
    "print('Found testing file.' if tf.io.gfile.exists(TEST_FILE_PATH) \n",
    "    else 'No testing file found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6167-3b53-4f83-9977-fb785e51eb66",
   "metadata": {},
   "source": [
    "### Export the imagery\n",
    "You can also export imagery using TFRecord format. Specifically, export whatever imagery you want to be classified by the trained model into the output Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ebe0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify patch and file dimensions.\n",
    "image_export_options = {\n",
    "  'patchDimensions': [256, 256],\n",
    "  'maxFileSize': 104857600,\n",
    "  'compressed': True\n",
    "}\n",
    "\n",
    "# Setup the task.\n",
    "image_task = ee.batch.Export.image.toCloudStorage(\n",
    "  image=image,\n",
    "  description='Image Export',\n",
    "  fileNamePrefix=IMAGE_FILE_PREFIX,\n",
    "  bucket=OUTPUT_BUCKET,\n",
    "  scale=30,\n",
    "  fileFormat='TFRecord',\n",
    "  region=EXPORT_REGION.toGeoJSON()['coordinates'],\n",
    "  formatOptions=image_export_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5dcef17-a093-4a1f-990e-2e69fd3d2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the task.\n",
    "image_task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "461704a9-c85f-4a11-8ea9-a50b3a600d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Task B7HDISVVYV7BF62DX2JHOS2E EXPORT_IMAGE: Image Export (COMPLETED)>,\n",
      " <Task A4XN4AFRVCZPDRAKH3UZAXU7 EXPORT_FEATURES: Testing Export (COMPLETED)>,\n",
      " <Task T3WR2ISKKUBAAANUACXJVMTJ EXPORT_FEATURES: Training Export (COMPLETED)>,\n",
      " <Task VEP6L2JTWSPDKQOIOHJHMQHI EXPORT_FEATURES: Testing Export (COMPLETED)>,\n",
      " <Task JCQAFODL2KTZLAGSMNH33JKE EXPORT_FEATURES: Training Export (COMPLETED)>]\n"
     ]
    }
   ],
   "source": [
    "# Monitor task progress\n",
    "pprint(ee.batch.Task.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a3a97-8b60-472e-b014-6266e0cd99a8",
   "metadata": {},
   "source": [
    "## Data preparation and pre-processing\n",
    "Read data from the TFRecord file into a tf.data.Dataset. Pre-process the dataset to get it into a suitable format for input to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf76f8-1898-413f-852e-8cf888134d32",
   "metadata": {},
   "source": [
    "### Read into a <code>tf.data.Dataset</code>\n",
    "Here we are going to read a file in Cloud Storage into a <code>tf.data.Dataset</code>. Check that you can read examples from the file. The purpose here is to ensure that we can read from the file without an error. The actual content is not necessarily human readable.\n",
    "\n",
    "> More on reading data into a <code>dataset</code>: https://www.tensorflow.org/guide/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "216871af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\nw\\n\\x0e\\n\\x02B2\\x12\\x08\\x12\\x06\\n\\x04\\x83QI=\\n\\x0e\\n\\x02B3\\x12\\x08\\x12\\x06\\n\\x04\\xa9\\xa4\\x8e=\\n\\x0e\\n\\x02B4\\x12\\x08\\x12\\x06\\n\\x04\\xd9\\xce\\xb7=\\n\\x0e\\n\\x02B5\\x12\\x08\\x12\\x06\\n\\x04\\xb3\\x0c1>\\n\\x0e\\n\\x02B6\\x12\\x08\\x12\\x06\\n\\x04\\xd0\\xb3Y>\\n\\x0e\\n\\x02B7\\x12\\x08\\x12\\x06\\n\\x04\\xd1\"\\x1b>\\n\\x15\\n\\tlandcover\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x80?', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 01:47:32.165872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:32.467833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:32.468101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:32.471820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 01:47:32.475002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:32.475221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:32.475382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:34.005209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:34.006394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:34.006588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-18 01:47:34.007433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6864 MB memory:  -> device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the TFRecord file in Cloud Storage.\n",
    "train_dataset = tf.data.TFRecordDataset(TRAIN_FILE_PATH, compression_type='GZIP')\n",
    "\n",
    "# Print the first record to check.\n",
    "print(iter(train_dataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156ff91-3ea6-44f9-91e1-c5ecb851d7f3",
   "metadata": {},
   "source": [
    "### Define the structure of your data\n",
    "For parsing the exported TFRecord files, <code>featuresDict</code> is a mapping between feature names (<code>featureNames</code> contains the band and label names) and <code>float32</code> <code>tf.io.FixedLenFeature</code> objects. This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors. \n",
    "\n",
    "Specifically, <b>all numeric data exported from Earth Engine is exported as <code>float32</code></b>.\n",
    "\n",
    "(Note: features in the TensorFlow context (i.e. <code>tf.train.Feature</code>) are not to be confused with Earth Engine features (i.e. <code>ee.Feature</code>), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2efee806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B2': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'B3': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'B4': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'B5': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'B6': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'B7': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
      " 'landcover': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "# List of fixed-length features, all of which are float32.\n",
    "columns = [\n",
    "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
    "]\n",
    "\n",
    "# Dictionary with names as keys, features as values.\n",
    "features_dict = dict(zip(FEATURE_NAMES, columns))\n",
    "\n",
    "pprint(features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4892a-8ee8-4855-96d1-5a47bf4e3346",
   "metadata": {},
   "source": [
    "### Parse the dataset\n",
    "Now we need to make a parsing function for the data in the TFRecord files. The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label. The parsing function reads data from a serialized <code>Example</code> proto into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example.\n",
    "\n",
    "> For an explanation on reading <code>Example</code> protos from TFRecord files: https://www.tensorflow.org/tutorials/load_data/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142e5a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'B2': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.04915], dtype=float32)>,\n",
      "  'B3': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.06965], dtype=float32)>,\n",
      "  'B4': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08975], dtype=float32)>,\n",
      "  'B5': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.1729], dtype=float32)>,\n",
      "  'B6': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.2126], dtype=float32)>,\n",
      "  'B7': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.1515], dtype=float32)>},\n",
      " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "\n",
    "  Read a serialized example into the structure defined by featuresDict.\n",
    "\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
    "  \"\"\"\n",
    "  parsed_features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "  labels = parsed_features.pop(LABEL)\n",
    "  return parsed_features, tf.cast(labels, tf.int32)\n",
    "\n",
    "# Map the function over the dataset.\n",
    "parsed_dataset = train_dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "\n",
    "# Print the first parsed record to check.\n",
    "pprint(iter(parsed_dataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c24e7-7533-462c-ad1c-e038abe1a7c8",
   "metadata": {},
   "source": [
    "Note that each record of the parsed dataset contains a tuple. The first element of the tuple is a dictionary with bands for keys and the numeric value of the bands for values. The second element of the tuple is a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bf6be-5405-4596-b03b-084964b8014e",
   "metadata": {},
   "source": [
    "### Create additional features\n",
    "Another thing we might want to do as part of the input process is to create new features, for example NDVI, a vegetation index computed from reflectance in two spectral bands. Here are some helper functions for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9da97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_difference(a, b):\n",
    "  \"\"\"Compute normalized difference of two inputs.\n",
    "\n",
    "  Compute (a - b) / (a + b).  If the denomenator is zero, add a small delta.\n",
    "\n",
    "  Args:\n",
    "    a: an input tensor with shape=[1]\n",
    "    b: an input tensor with shape=[1]\n",
    "\n",
    "  Returns:\n",
    "    The normalized difference as a tensor.\n",
    "  \"\"\"\n",
    "  nd = (a - b) / (a + b)\n",
    "  nd_inf = (a - b) / (a + b + 0.000001)\n",
    "  return tf.where(tf.math.is_finite(nd), nd, nd_inf)\n",
    "\n",
    "def add_NDVI(features, label):\n",
    "  \"\"\"Add NDVI to the dataset.\n",
    "  Args:\n",
    "    features: a dictionary of input tensors keyed by feature name.\n",
    "    label: the target label\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the input dictionary with an NDVI tensor added and the label.\n",
    "  \"\"\"\n",
    "  features['NDVI'] = normalized_difference(features['B5'], features['B4'])\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d352f",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "The basic workflow for classification in TensorFlow is:\n",
    "\n",
    "1. Create the model.\n",
    "2. Train the model (i.e. <code>fit()</code>).\n",
    "3. Use the trained model for inference (i.e. <code>predict()</code>).\n",
    "\n",
    "Here we'll create a Sequential neural network model using Keras. This simple model is inspired by examples in:\n",
    "\n",
    "> The TensorFlow Get Started tutorial: https://www.tensorflow.org/tutorials/\n",
    "\n",
    "> The TensorFlow Keras guide: https://www.tensorflow.org/guide/keras#build_a_simple_model\n",
    "\n",
    "> The Keras <code>Sequential</code> model examples: https://keras.io/guides/sequential_model/\n",
    "\n",
    "Note that the model used here is purely for demonstration purposes and hasn't gone through any performance tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d4789-10ad-4ab7-8acf-50c31b46d4d1",
   "metadata": {},
   "source": [
    "## Create the Keras model\n",
    "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss. Specifically, Keras expects a list of inputs and a one-hot vector for the class.\n",
    "\n",
    "> https://keras.io/api/losses/\n",
    "\n",
    "> https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "\n",
    "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer. Once the dataset has been prepared, define the model, compile it, fit it to the training data.\n",
    "\n",
    "> This is the model we will use: https://keras.io/guides/sequential_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d655306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 01:48:22.782531: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f1704005cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-18 01:48:22.782573: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2023-09-18 01:48:22.828504: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-18 01:48:23.264590: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 42ms/step - loss: 1.0900 - accuracy: 0.3571\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0452 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0041 - accuracy: 0.7714\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9702 - accuracy: 0.9143\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9278 - accuracy: 0.9143\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8966 - accuracy: 0.9571\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8679 - accuracy: 0.9143\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8232 - accuracy: 0.9286\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7955 - accuracy: 0.9143\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7662 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18280d1c90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Add NDVI.\n",
    "input_dataset = parsed_dataset.map(add_NDVI)\n",
    "\n",
    "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
    "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
    "# the label needs to be turned into a one-hot vector.\n",
    "def to_tuple(inputs, label):\n",
    "  return (tf.transpose(list(inputs.values())),\n",
    "          tf.one_hot(indices=label, depth=N_CLASSES))\n",
    "\n",
    "# Map the to_tuple function, shuffle and batch.\n",
    "input_dataset = input_dataset.map(to_tuple).batch(8)\n",
    "\n",
    "# Define the layers in the model.\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(N_CLASSES, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the model with the specified loss function.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data.\n",
    "model.fit(x=input_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e25623-57a5-40ae-b37e-828683fab594",
   "metadata": {},
   "source": [
    "### Check model accuracy on the test set\n",
    "Now that we have a trained model, we can evaluate it using the test dataset. To do that, read and prepare the test dataset in the same way as the training dataset. Here we specify a batch size of 1 so that each example in the test set is used exactly once to compute model accuracy. For model steps, just specify a number larger than the test dataset size (ignore the warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2633c380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 3ms/step - loss: 0.8110 - accuracy: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8110493421554565, 0.7857142686843872]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = (\n",
    "  tf.data.TFRecordDataset(TEST_FILE_PATH, compression_type='GZIP')\n",
    "    .map(parse_tfrecord, num_parallel_calls=5)\n",
    "    .map(add_NDVI)\n",
    "    .map(to_tuple)\n",
    "    .batch(1))\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d4837-a27e-4d13-86fa-065c4a6e17a1",
   "metadata": {},
   "source": [
    "Two metrics of loss and accuracy are output here:\n",
    "- Loss: monitor it during training to see if it is increasing, decreasing, staying the same. Indicates if have hit a minima/if you are optimised or not.\n",
    "- Accuracy: monitor it during the test-validate phase as a measure of overall outcome performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7d7c2-3d66-48c0-9fd4-9f3d9859a88c",
   "metadata": {},
   "source": [
    "## Use the trained model to classify an image from Earth Engine\n",
    "Now it's time to classify the image that was exported from Earth Engine. If the exported image is large, it will be split into multiple TFRecord files in its destination folder. There will also be a JSON sidecar file called \"the mixer\" that describes the format and georeferencing of the image. Here we will find the image files and the mixer file, getting some info out of the mixer that will be useful during model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7453da0-ea5a-46aa-92ba-d71958990e6d",
   "metadata": {},
   "source": [
    "### Find the image files and JSON mixer file in Cloud Storage\n",
    "Use <code>gsutil</code> to locate the files of interest in the output Cloud Storage bucket. Check to make sure your image export task finished before running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48a3bdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://ml-bucket-dsmi800/Image_pixel_demo_00000.tfrecord.gz',\n",
      " 'gs://ml-bucket-dsmi800/Image_pixel_demo_00001.tfrecord.gz']\n",
      "gs://ml-bucket-dsmi800/Image_pixel_demo_mixer.json\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the files in the output bucket.\n",
    "files_list = !gsutil ls 'gs://'{OUTPUT_BUCKET}\n",
    "# Get only the files generated by the image export.\n",
    "exported_files_list = [s for s in files_list if IMAGE_FILE_PREFIX in s]\n",
    "\n",
    "# Get the list of image files and the JSON mixer file.\n",
    "image_files_list = []\n",
    "json_file = None\n",
    "for f in exported_files_list:\n",
    "  if f.endswith('.tfrecord.gz'):\n",
    "    image_files_list.append(f)\n",
    "  elif f.endswith('.json'):\n",
    "    json_file = f\n",
    "\n",
    "# Make sure the files are in the right order.\n",
    "image_files_list.sort()\n",
    "\n",
    "pprint(image_files_list)\n",
    "print(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a25fdc-45c7-4eac-a601-93c1da1356d9",
   "metadata": {},
   "source": [
    "### Read the JSON mixer file\n",
    "The mixer contains metadata and georeferencing information for the exported patches, each of which is in a different file. Read the mixer to get some information needed for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a6dd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patchDimensions': [256, 256],\n",
      " 'patchesPerRow': 13,\n",
      " 'projection': {'affine': {'doubleMatrix': [0.00026949458523585647,\n",
      "                                            0.0,\n",
      "                                            -122.70007617412975,\n",
      "                                            0.0,\n",
      "                                            -0.00026949458523585647,\n",
      "                                            38.00089247493765]},\n",
      "                'crs': 'EPSG:4326'},\n",
      " 'totalPatches': 130}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the contents of the mixer file to a JSON object.\n",
    "json_text = !gsutil cat {json_file}\n",
    "\n",
    "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "mixer = json.loads(json_text.nlstr)\n",
    "pprint(mixer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9f4c4-6a01-4221-b320-c564aee857a0",
   "metadata": {},
   "source": [
    "### Read the image files into a dataset\n",
    "You can feed the list of files (<code>imageFilesList</code>) directly to the <code>TFRecordDataset</code> constructor to make a combined dataset on which to perform inference. The input needs to be preprocessed differently than the training and testing. Mainly, this is because the pixels are written into records as patches, we need to read the patches in as one big tensor (one patch for each band), then flatten them into lots of little tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c06b7-ace8-4281-b009-13f58dafcc6b",
   "metadata": {},
   "source": [
    "Tensors are the data structure used by machine learning systems. Three properties define a tensor:\n",
    "- range\n",
    "- shape\n",
    "- dType: Here the rank of a tensor refers to the number of axes of the tensor.\n",
    "\n",
    "> https://furkangulsen.medium.com/what-is-a-tensor-ce8e78835d08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b20c971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant info from the JSON mixer file.\n",
    "patch_width = mixer['patchDimensions'][0]\n",
    "patch_height = mixer['patchDimensions'][1]\n",
    "patches = mixer['totalPatches']\n",
    "patch_dimensions_flat = [patch_width * patch_height, 1]\n",
    "\n",
    "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
    "image_columns = [\n",
    "  tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) \n",
    "    for k in BANDS\n",
    "]\n",
    "\n",
    "# Parsing dictionary.\n",
    "image_features_dict = dict(zip(BANDS, image_columns))\n",
    "\n",
    "# Note that you can make one dataset from many files by specifying a list.\n",
    "image_dataset = tf.data.TFRecordDataset(image_files_list, compression_type='GZIP')\n",
    "\n",
    "# Parsing function.\n",
    "def parse_image(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, image_features_dict)\n",
    "\n",
    "# Parse the data into tensors, one long tensor per patch.\n",
    "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "\n",
    "# Break our long tensors into many little ones.\n",
    "image_dataset = image_dataset.flat_map(\n",
    "  lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
    ")\n",
    "\n",
    "# Add additional features (NDVI).\n",
    "image_dataset = image_dataset.map(\n",
    "  # Add NDVI to a feature that doesn't have a label.\n",
    "  lambda features: add_NDVI(features, None)[0]\n",
    ")\n",
    "\n",
    "# Turn the dictionary in each record into a tuple without a label.\n",
    "image_dataset = image_dataset.map(\n",
    "  lambda data_dict: (tf.transpose(list(data_dict.values())), )\n",
    ")\n",
    "\n",
    "# Turn each patch into a batch.\n",
    "image_dataset = image_dataset.batch(patch_width * patch_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478e6f6-f0db-45f1-9210-81ca3ccc7b8f",
   "metadata": {},
   "source": [
    "### Generate predictions for the image pixels\n",
    "To get predictions in each pixel, run the image dataset through the trained model using <code>model.predict()</code>. Print the first prediction to see that the output is a list of the three class probabilities for each pixel. Running all predictions might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51ae4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 192s 1s/step\n",
      "[[0.2521426  0.6080544  0.13980293]]\n"
     ]
    }
   ],
   "source": [
    "# Run prediction in batches, with as many steps as there are patches.\n",
    "predictions = model.predict(image_dataset, steps=patches, verbose=1)\n",
    "\n",
    "# Note that the predictions come as a numpy array.  Check the first one.\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2526bd8-6099-49bd-aedb-2b3d8910e209",
   "metadata": {},
   "source": [
    "### Write the predictions to a TFRecord file\n",
    "Now that there's a list of class probabilities in <code>predictions</code>, it's time to write them back into a file, optionally including a class label which is simply the index of the maximum probability. We'll write directly from TensorFlow to a file in the output Cloud Storage bucket.\n",
    "\n",
    "Iterate over the list, compute class label and write the class and the probabilities in patches. Specifically, we need to write the pixels into the file as patches in the same order they came out. The records are written as serialized <code>tf.train.Example</code> protos. This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cc0d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file gs://ml-bucket-dsmi800/Classified_pixel_demo.TFRecord\n",
      "Done with patch 1 of 130...\n",
      "Done with patch 2 of 130...\n",
      "Done with patch 3 of 130...\n",
      "Done with patch 4 of 130...\n",
      "Done with patch 5 of 130...\n",
      "Done with patch 6 of 130...\n",
      "Done with patch 7 of 130...\n",
      "Done with patch 8 of 130...\n",
      "Done with patch 9 of 130...\n",
      "Done with patch 10 of 130...\n",
      "Done with patch 11 of 130...\n",
      "Done with patch 12 of 130...\n",
      "Done with patch 13 of 130...\n",
      "Done with patch 14 of 130...\n",
      "Done with patch 15 of 130...\n",
      "Done with patch 16 of 130...\n",
      "Done with patch 17 of 130...\n",
      "Done with patch 18 of 130...\n",
      "Done with patch 19 of 130...\n",
      "Done with patch 20 of 130...\n",
      "Done with patch 21 of 130...\n",
      "Done with patch 22 of 130...\n",
      "Done with patch 23 of 130...\n",
      "Done with patch 24 of 130...\n",
      "Done with patch 25 of 130...\n",
      "Done with patch 26 of 130...\n",
      "Done with patch 27 of 130...\n",
      "Done with patch 28 of 130...\n",
      "Done with patch 29 of 130...\n",
      "Done with patch 30 of 130...\n",
      "Done with patch 31 of 130...\n",
      "Done with patch 32 of 130...\n",
      "Done with patch 33 of 130...\n",
      "Done with patch 34 of 130...\n",
      "Done with patch 35 of 130...\n",
      "Done with patch 36 of 130...\n",
      "Done with patch 37 of 130...\n",
      "Done with patch 38 of 130...\n",
      "Done with patch 39 of 130...\n",
      "Done with patch 40 of 130...\n",
      "Done with patch 41 of 130...\n",
      "Done with patch 42 of 130...\n",
      "Done with patch 43 of 130...\n",
      "Done with patch 44 of 130...\n",
      "Done with patch 45 of 130...\n",
      "Done with patch 46 of 130...\n",
      "Done with patch 47 of 130...\n",
      "Done with patch 48 of 130...\n",
      "Done with patch 49 of 130...\n",
      "Done with patch 50 of 130...\n",
      "Done with patch 51 of 130...\n",
      "Done with patch 52 of 130...\n",
      "Done with patch 53 of 130...\n",
      "Done with patch 54 of 130...\n",
      "Done with patch 55 of 130...\n",
      "Done with patch 56 of 130...\n",
      "Done with patch 57 of 130...\n",
      "Done with patch 58 of 130...\n",
      "Done with patch 59 of 130...\n",
      "Done with patch 60 of 130...\n",
      "Done with patch 61 of 130...\n",
      "Done with patch 62 of 130...\n",
      "Done with patch 63 of 130...\n",
      "Done with patch 64 of 130...\n",
      "Done with patch 65 of 130...\n",
      "Done with patch 66 of 130...\n",
      "Done with patch 67 of 130...\n",
      "Done with patch 68 of 130...\n",
      "Done with patch 69 of 130...\n",
      "Done with patch 70 of 130...\n",
      "Done with patch 71 of 130...\n",
      "Done with patch 72 of 130...\n",
      "Done with patch 73 of 130...\n",
      "Done with patch 74 of 130...\n",
      "Done with patch 75 of 130...\n",
      "Done with patch 76 of 130...\n",
      "Done with patch 77 of 130...\n",
      "Done with patch 78 of 130...\n",
      "Done with patch 79 of 130...\n",
      "Done with patch 80 of 130...\n",
      "Done with patch 81 of 130...\n",
      "Done with patch 82 of 130...\n",
      "Done with patch 83 of 130...\n",
      "Done with patch 84 of 130...\n",
      "Done with patch 85 of 130...\n",
      "Done with patch 86 of 130...\n",
      "Done with patch 87 of 130...\n",
      "Done with patch 88 of 130...\n",
      "Done with patch 89 of 130...\n",
      "Done with patch 90 of 130...\n",
      "Done with patch 91 of 130...\n",
      "Done with patch 92 of 130...\n",
      "Done with patch 93 of 130...\n",
      "Done with patch 94 of 130...\n",
      "Done with patch 95 of 130...\n",
      "Done with patch 96 of 130...\n",
      "Done with patch 97 of 130...\n",
      "Done with patch 98 of 130...\n",
      "Done with patch 99 of 130...\n",
      "Done with patch 100 of 130...\n",
      "Done with patch 101 of 130...\n",
      "Done with patch 102 of 130...\n",
      "Done with patch 103 of 130...\n",
      "Done with patch 104 of 130...\n",
      "Done with patch 105 of 130...\n",
      "Done with patch 106 of 130...\n",
      "Done with patch 107 of 130...\n",
      "Done with patch 108 of 130...\n",
      "Done with patch 109 of 130...\n",
      "Done with patch 110 of 130...\n",
      "Done with patch 111 of 130...\n",
      "Done with patch 112 of 130...\n",
      "Done with patch 113 of 130...\n",
      "Done with patch 114 of 130...\n",
      "Done with patch 115 of 130...\n",
      "Done with patch 116 of 130...\n",
      "Done with patch 117 of 130...\n",
      "Done with patch 118 of 130...\n",
      "Done with patch 119 of 130...\n",
      "Done with patch 120 of 130...\n",
      "Done with patch 121 of 130...\n",
      "Done with patch 122 of 130...\n",
      "Done with patch 123 of 130...\n",
      "Done with patch 124 of 130...\n",
      "Done with patch 125 of 130...\n",
      "Done with patch 126 of 130...\n",
      "Done with patch 127 of 130...\n",
      "Done with patch 128 of 130...\n",
      "Done with patch 129 of 130...\n",
      "Done with patch 130 of 130...\n",
      "Writing complete!\n"
     ]
    }
   ],
   "source": [
    "### WARNING! Not tested successfully!! ###\n",
    "\n",
    "print('Writing to file ' + OUTPUT_IMAGE_FILE)\n",
    "\n",
    "# Instantiate the writer.\n",
    "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n",
    "\n",
    "# Every patch-worth of predictions we'll dump an example into the output\n",
    "# file with a single feature that holds our predictions. Since our predictions\n",
    "# are already in the order of the exported data, the patches we create here\n",
    "# will also be in the right order.\n",
    "patch = [[], [], [], []]\n",
    "cur_patch = 1\n",
    "for prediction in predictions:\n",
    "    patch[0].append(tf.argmax(prediction[0]))\n",
    "    patch[1].append(prediction[0][0])\n",
    "    patch[2].append(prediction[0][1])\n",
    "    patch[3].append(prediction[0][2])\n",
    "    \n",
    "    # Once we've seen a patches-worth of class_ids...\n",
    "    if (len(patch[0]) == patch_width * patch_height):\n",
    "        print('Done with patch ' + str(cur_patch) + ' of ' + str(patches) + '...')\n",
    "        # Create an example\n",
    "        example = tf.train.Example(\n",
    "          features=tf.train.Features(\n",
    "            feature={\n",
    "              'prediction': tf.train.Feature(\n",
    "                  int64_list=tf.train.Int64List(\n",
    "                      value=patch[0])),\n",
    "              'bareProb': tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(\n",
    "                      value=patch[1])),\n",
    "              'vegProb': tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(\n",
    "                      value=patch[2])),\n",
    "              'waterProb': tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(\n",
    "                      value=patch[3])),\n",
    "            }\n",
    "          )\n",
    "        )\n",
    "        # Write the example to the file and clear our patch array so it's ready for\n",
    "        # another batch of class ids\n",
    "        writer.write(example.SerializeToString())\n",
    "        patch = [[], [], [], []]\n",
    "        cur_patch += 1\n",
    "\n",
    "writer.close()\n",
    "print('Writing complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c565b6-1618-41a6-92b6-5c2ac3d2a624",
   "metadata": {},
   "source": [
    "## Upload the classifications to an Earth Engine asset\n",
    "### Verify the existence of the predictions file\n",
    "At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket. Use the gsutil command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4af00a97-3bcb-48a1-8fae-a75a2361247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110772220  2023-09-18T03:22:07Z  gs://ml-bucket-dsmi800/Classified_pixel_demo.TFRecord\n",
      "TOTAL: 1 objects, 110772220 bytes (105.64 MiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l {OUTPUT_IMAGE_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bdb4f-f9ef-481d-ad8d-4b53661d3d0f",
   "metadata": {},
   "source": [
    "### Upload the classified image to Earth Engine\n",
    "Upload the image to Earth Engine directly from the Cloud Storage bucket with the <code>earthengine</code> command. Provide both the image TFRecord file and the JSON file as arguments to <code>earthengine upload</code>.\n",
    "    \n",
    "    > https://developers.google.com/earth-engine/command_line#upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e2228e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to users/dsmi800/Classified_pixel_demo\n",
      "Please authorize access to your Earth Engine account by running\n",
      "\n",
      "earthengine authenticate\n",
      "\n",
      "in your command line, and then retry.\n"
     ]
    }
   ],
   "source": [
    "print('Uploading to ' + OUTPUT_ASSET_ID)\n",
    "\n",
    "# Start the upload.\n",
    "!earthengine upload image --asset_id={OUTPUT_ASSET_ID} --pyramiding_policy=mode {OUTPUT_IMAGE_FILE} {json_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab8e13-abee-4122-863a-c1cb46379138",
   "metadata": {},
   "source": [
    "### Check the status of the asset ingestion\n",
    "You can also use the Earth Engine API to check the status of your asset upload. It might take a while. The upload of the image is an asset ingestion task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7750-1bb6-449c-b82b-1e83dce66914",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.batch.Task.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05812c3f-ef39-4525-986c-9463371d2dc2",
   "metadata": {},
   "source": [
    "### View the ingested asset\n",
    "Display the vector of class probabilities as an RGB image with colors corresponding to the probability of bare, vegetation, water in a pixel. Also display the winning class using the same color palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_image = ee.Image(OUTPUT_ASSET_ID)\n",
    "\n",
    "prediction_vis = {\n",
    "  'bands': 'prediction',\n",
    "  'min': 0,\n",
    "  'max': 2,\n",
    "  'palette': ['red', 'green', 'blue']\n",
    "}\n",
    "probability_vis = {'bands': ['bareProb', 'vegProb', 'waterProb'], 'max': 0.5}\n",
    "\n",
    "prediction_map_id = predictions_image.getMapId(prediction_vis)\n",
    "probability_map_id = predictions_image.getMapId(probability_vis)\n",
    "\n",
    "map = folium.Map(location=[37.6413, -122.2582])\n",
    "folium.TileLayer(\n",
    "  tiles=prediction_map_id['tile_fetcher'].url_format,\n",
    "  attr='Map Data © Google Earth Engine',\n",
    "  overlay=True,\n",
    "  name='prediction',\n",
    ").add_to(map)\n",
    "folium.TileLayer(\n",
    "  tiles=probability_map_id['tile_fetcher'].url_format,\n",
    "  attr='Map Data © Google Earth Engine',\n",
    "  overlay=True,\n",
    "  name='probability',\n",
    ").add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
